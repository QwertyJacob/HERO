{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3abcf821-3720-4e4d-a04d-caf64382fa36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjfcevallos\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "os.environ['HTTP_PROXY'] = 'http://proxy.uninsubria.it:3128/'\n",
    "os.environ['HTTPS_PROXY'] = 'http://proxy.uninsubria.it:3128/'\n",
    "wandb.login()\n",
    "wb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba021fcf-a7aa-41db-a8fe-0512dc9863e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scapy -qU\n",
    "# !pip install pcap-splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d4b63d-6066-4ea7-b56c-6dd6aaffe68d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "006763e4-e871-4b46-9ebc-2b43aa658019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scapy.all import *\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import binascii\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d26c65d-25fe-4104-90a6-fcd1383979d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_flow_details_dataset(flow_pcap_dir, max_records=10000):\n",
    "\n",
    "    # Initialize an empty list to store flow information\n",
    "    flow_info = []\n",
    "\n",
    "    # Iterate through the PCAP files in the directory\n",
    "    for pcap_file in tqdm(os.listdir(flow_pcap_dir)[:max_records]):\n",
    "        if pcap_file.endswith(\".pcap\"):\n",
    "            pcap_file_path = os.path.join(flow_pcap_dir, pcap_file)\n",
    "\n",
    "            # Use Scapy to read the PCAP file\n",
    "            packets = rdpcap(pcap_file_path)\n",
    "\n",
    "            for packet in packets:\n",
    "                if IP in packet:\n",
    "                    ip_layer = packet[IP]\n",
    "\n",
    "                    if TCP in packet:\n",
    "                        transport_layer = packet[TCP]\n",
    "                        protocol = \"TCP\"\n",
    "                    elif UDP in packet:\n",
    "                        transport_layer = packet[UDP]\n",
    "                        protocol = \"UDP\"\n",
    "                    else:\n",
    "                        transport_layer = None\n",
    "                        protocol = None\n",
    "\n",
    "                    # Extract relevant fields\n",
    "                    time_stamp = datetime.fromtimestamp(int(packet.time)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    source_ip = ip_layer.src\n",
    "                    dest_ip = ip_layer.dst\n",
    "                    source_port = transport_layer.sport if transport_layer else None\n",
    "                    dest_port = transport_layer.dport if transport_layer else None\n",
    "\n",
    "                    flow_info.append({\n",
    "                        \"filename\": pcap_file,\n",
    "                        \"stime\": time_stamp,\n",
    "                        \"saddr\": source_ip,\n",
    "                        \"sport\": source_port,\n",
    "                        \"daddr\": dest_ip,\n",
    "                        \"dport\": dest_port,\n",
    "                        \"proto\": protocol\n",
    "                    })\n",
    "                    break\n",
    "\n",
    "    # Create a Pandas DataFrame from the flow information\n",
    "    df = pd.DataFrame(flow_info)\n",
    "    return df\n",
    "\n",
    "\n",
    "def to_fixed_length(binary_data, fixed_length=512):\n",
    "    current_length = len(binary_data)\n",
    "\n",
    "    if current_length >= fixed_length:\n",
    "        return binary_data[:fixed_length]\n",
    "\n",
    "    # Calculate the number of zeros to add\n",
    "    num_zeros_to_add = fixed_length - current_length\n",
    "\n",
    "    # Create an array of zeros to pad\n",
    "    padding = np.zeros(num_zeros_to_add, dtype=np.uint8)\n",
    "\n",
    "    # Concatenate the padding to the end of the binary_data\n",
    "    padded_data = np.concatenate((binary_data, padding))\n",
    "\n",
    "    return padded_data\n",
    "\n",
    "\n",
    "def process_pcap_file(file_path, flow_len=512, packet_len=512):\n",
    "\n",
    "    packets = rdpcap(file_path)\n",
    "    np_packets = []\n",
    "    for packet in packets[:flow_len]:\n",
    "        if packet.haslayer(IP):\n",
    "            # we take from layer 3 on:\n",
    "            packet = packet[IP]\n",
    "            # And we mask the ip addressese\n",
    "            packet.src = \"0.0.0.0\"\n",
    "            packet.dst = \"0.0.0.0\"\n",
    "            # We also mask the ports\n",
    "            packet.soprt = 00000\n",
    "            packet.dport = 00000\n",
    "            # Convert packet to bytes\n",
    "            packet = bytes(packet)\n",
    "            # Convert bytes to numpy array of uint8\n",
    "            packet = np.frombuffer(packet, dtype=np.uint8)\n",
    "            # pad\n",
    "            packet = to_fixed_length(packet, packet_len)\n",
    "            np_packets.append(packet.reshape(1, -1))\n",
    "\n",
    "    if len(np_packets) > 0:\n",
    "        flow = np.concatenate(np_packets, 0)\n",
    "        if flow.shape[0] < flow_len:\n",
    "            pad = np.zeros((flow_len-flow.shape[0], packet_len))\n",
    "            flow = np.concatenate([flow, pad], 0)\n",
    "\n",
    "        return flow[:flow_len]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_flow_labels_df(pf_path):\n",
    "    original_labels = pd.read_csv(\n",
    "        pf_path,\n",
    "        sep=';',\n",
    "        low_memory=False)\n",
    "\n",
    "    dates = pd.to_datetime(original_labels.stime, unit='s')\n",
    "\n",
    "    original_labels.stime = dates.dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    original_labels['sport'] = pd.to_numeric(original_labels['sport'], errors='coerce')\n",
    "    original_labels['dport'] = pd.to_numeric(original_labels['dport'], errors='coerce')\n",
    "    original_labels.dropna(subset=['sport', 'dport'], inplace=True)\n",
    "    original_labels = original_labels.astype({\"sport\":\"int\",\"dport\":\"int\"})\n",
    "    return original_labels\n",
    "\n",
    "\n",
    "def get_data(flow_path, max_flows, normal_flow_filenames):\n",
    "    attack_flow_df = []\n",
    "    attack_flows_tensor = []\n",
    "\n",
    "    normal_flow_df = []\n",
    "    normal_flows_tensor = []\n",
    "\n",
    "    for index, filename in tqdm(enumerate(os.listdir(flow_path))):\n",
    "        if filename.endswith(\".pcap\"):\n",
    "            file_path = os.path.join(flow_path, filename)\n",
    "            flow_array = process_pcap_file(\n",
    "                file_path,\n",
    "                flow_len=512,\n",
    "                packet_len=512)\n",
    "            if flow_array is not None:\n",
    "                flow_array = flow_array[np.newaxis, :]\n",
    "                if filename in normal_flow_filenames:\n",
    "                    normal_flow_df.append(\n",
    "                        {'tensor_index': index,\n",
    "                         'filename': filename})\n",
    "                    normal_flows_tensor.append(torch.from_numpy(flow_array))\n",
    "                else:\n",
    "                    attack_flow_df.append(\n",
    "                        {'tensor_index': index,\n",
    "                         'filename': filename})\n",
    "                    attack_flows_tensor.append(torch.from_numpy(flow_array))\n",
    "        if index >= max_flows:\n",
    "            break\n",
    "\n",
    "    attack_flow_df = pd.DataFrame(attack_flow_df)\n",
    "    normal_flow_df = pd.DataFrame(normal_flow_df)\n",
    "\n",
    "    return attack_flow_df, attack_flows_tensor, normal_flow_df, normal_flows_tensor\n",
    "\n",
    "\n",
    "def process_and_save_data(\n",
    "        flow_path,\n",
    "        max_flows,\n",
    "        normal_flow_filenames,\n",
    "        imgs_path,\n",
    "        attack_name\n",
    "        ):\n",
    "\n",
    "    for index, filename in tqdm(enumerate(os.listdir(flow_path))):\n",
    "        if filename.endswith(\".pcap\"):\n",
    "            file_path = os.path.join(flow_path, filename)\n",
    "            flow_array = process_pcap_file(\n",
    "                file_path,\n",
    "                flow_len=512,\n",
    "                packet_len=512)\n",
    "            if flow_array is not None:\n",
    "                flow_array = flow_array[np.newaxis, :]\n",
    "                if filename in normal_flow_filenames:\n",
    "                    torch.save(\n",
    "                        torch.from_numpy(flow_array),\n",
    "                        f'Normal_from_{attack_name}_{index}')\n",
    "                else:\n",
    "                    torch.save(\n",
    "                        torch.from_numpy(flow_array),\n",
    "                        f'{imgs_path}/{attack_name}_{index}')\n",
    "        if index >= max_flows:\n",
    "            break\n",
    "\n",
    "\n",
    "def process_and_save_data_opt(\n",
    "        flow_path,\n",
    "        max_flows,\n",
    "        imgs_path,\n",
    "        attack_name\n",
    "        ):\n",
    "\n",
    "    for index, filename in tqdm(enumerate(os.listdir(flow_path))):\n",
    "        if filename.endswith(\".pcap\"):\n",
    "            file_path = os.path.join(flow_path, filename)\n",
    "            flow_array = process_pcap_file(\n",
    "                file_path,\n",
    "                flow_len=512,\n",
    "                packet_len=512)\n",
    "            if flow_array is not None:\n",
    "                torch.save(\n",
    "                    torch.from_numpy(flow_array),\n",
    "                    f'{imgs_path}/{attack_name}_{index}')\n",
    "        if index >= max_flows:\n",
    "            break\n",
    "\n",
    "\n",
    "def process_and_save_image_opt(\n",
    "        flow_path,\n",
    "        max_flows,\n",
    "        imgs_path,\n",
    "        attack_name\n",
    "        ):\n",
    "\n",
    "    for index, filename in tqdm(enumerate(os.listdir(flow_path))):\n",
    "        if filename.endswith(\".pcap\"):\n",
    "            file_path = os.path.join(flow_path, filename)\n",
    "            flow_array = process_pcap_file(\n",
    "                file_path,\n",
    "                flow_len=512,\n",
    "                packet_len=512)\n",
    "            if flow_array is not None:\n",
    "                image = Image.fromarray(flow_array)\n",
    "                image = image.convert(\"L\")\n",
    "                image.save(f'{imgs_path}/{attack_name}_{index}.png')\n",
    "        if index >= max_flows:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a48e3d-9bff-4c5f-854e-94bc7066bdeb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data (bulk):\n",
    "______\n",
    "\n",
    "In this script we concatenate all the bidimensional tensors of each attack into a huge tensor and save it into a file... these files are saved into the nfs directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56760be4-9b26-4fc5-b999-03cc7049147f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fa8f5e666e4aab85c3bc871e9fc50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading labels file...\n",
      "merging metadata 1...\n",
      "querying normal flows...\n",
      "processing pcaps...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be8470bc552424db88d472be73f1226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WORKSPACE_DIR =  '/home/jovyan/nfs/jcevallos/datasets/'\n",
    "# !tar -xzvf {datafile} -C {WORKSPACE_DIR}\n",
    "datadir = WORKSPACE_DIR + 'BoT_IoT_raw/'\n",
    "normal_traffic_dir = datadir + 'Normal/'\n",
    "\n",
    "# CHANGE THIS FOR EACH ATTACK:\n",
    "macro_attack_str = 'Theft'\n",
    "attack_str = 'Data_Exfiltration'\n",
    "max_flows = 10000\n",
    "\n",
    "attack_dir = datadir + f'{macro_attack_str}/{attack_str}/'\n",
    "flow_path = attack_dir + 'flows'\n",
    "\n",
    "# first, read flow details\n",
    "flow_info_ds = get_flow_details_dataset(\n",
    "    flow_path,\n",
    "    max_records=max_flows)\n",
    "\n",
    "# Then, read the labels\n",
    "print('reading labels file...')\n",
    "flow_labels_df = get_flow_labels_df(\n",
    "    attack_dir+f'{attack_str}.csv')\n",
    "\n",
    "# merge details and labels\n",
    "print('merging metadata 1...')\n",
    "merged_flows = pd.merge(flow_info_ds,\n",
    "                        flow_labels_df,\n",
    "                        on=['stime',\n",
    "                            'saddr',\n",
    "                            'sport',\n",
    "                            'daddr',\n",
    "                            'dport'])\n",
    "\n",
    "# get normal_flow_filenames:\n",
    "print('querying normal flows...')\n",
    "normal_flow_filenames = merged_flows.filename[merged_flows['category'] == 'Normal']\n",
    "\n",
    "# get data:\n",
    "print('processing pcaps...')\n",
    "a_flow_df, a_flows_tensor, n_flow_df, n_flows_tensor = get_data(\n",
    "                                                        flow_path,\n",
    "                                                        max_flows,\n",
    "                                                        normal_flow_filenames)\n",
    "\n",
    "# save attack flow tensors\n",
    "print('saving tensor file...')\n",
    "torch.save(a_flows_tensor,\n",
    "           attack_dir + 'flows_tensor.pt')\n",
    "\n",
    "# add metadata:\n",
    "print('merging metadata 2...')\n",
    "a_flow_df = pd.merge(\n",
    "         a_flow_df,\n",
    "         merged_flows,\n",
    "         on=['filename'])\n",
    "\n",
    "# save attack metadata:\n",
    "print('saving attack metadata file...')\n",
    "a_flow_df.to_csv(\n",
    "    attack_dir+f'{attack_str}_flow_metadata.csv',\n",
    "    index=False)\n",
    "\n",
    "# check for normal data:\n",
    "if len(n_flow_df) > 0:\n",
    "    print('processing normal data...')\n",
    "    # save normal flow tensors\n",
    "    torch.save(\n",
    "        n_flows_tensor,\n",
    "        normal_traffic_dir + f'Normal_from_{attack_str}.pt')\n",
    "\n",
    "    # add metadata\n",
    "    n_flow_df = pd.merge(\n",
    "             n_flow_df,\n",
    "             merged_flows,\n",
    "             on=['filename'])\n",
    "\n",
    "    # save normal metadata:\n",
    "    n_flow_df.to_csv(\n",
    "        normal_traffic_dir+f'normal_from_{attack_str}_flow_metadata.csv')\n",
    "\n",
    "print('preprocessing completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e33a90-ffb0-4541-a5ab-bf8581323fe1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data (file-wise):\n",
    "______\n",
    "\n",
    "in this script we save each image in a file, where the name of the file gives info about the label. And we save all the images of every attack in the same folder in the shared folder for fast access during training in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc71a52-6d80-4edd-905c-51307c5ae747",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_DIR =  '/home/jovyan/nfs/jcevallos/datasets/'\n",
    "# !tar -xzvf {datafile} -C {WORKSPACE_DIR}\n",
    "datadir = WORKSPACE_DIR + 'BoT_IoT_raw/'\n",
    "normal_traffic_dir = datadir + 'Normal/'\n",
    "images_path = '/home/jovyan/shared/jesus/datasets/BoT_IoT_imgs/'\n",
    "# CHANGE THIS FOR EACH ATTACK:\n",
    "macro_attack_str = 'DDoS'\n",
    "attack_str = 'DDoS_HTTP'\n",
    "max_flows = 10000\n",
    "\n",
    "attack_dir = datadir + f'{macro_attack_str}/{attack_str}/'\n",
    "flow_path = attack_dir + 'flows'\n",
    "\n",
    "# first, read flow details\n",
    "print('getting flow details...')\n",
    "flow_info_ds = get_flow_details_dataset(\n",
    "    flow_path,\n",
    "    max_records=max_flows)\n",
    "\n",
    "# Then, read the labels\n",
    "print('reading labels file...')\n",
    "flow_labels_df = get_flow_labels_df(\n",
    "    attack_dir+f'{attack_str}.csv')\n",
    "\n",
    "# merge details and labels\n",
    "print('merging metadata 1...')\n",
    "merged_flows = pd.merge(flow_info_ds,\n",
    "                        flow_labels_df,\n",
    "                        on=['stime',\n",
    "                            'saddr',\n",
    "                            'sport',\n",
    "                            'daddr',\n",
    "                            'dport'])\n",
    "\n",
    "# get normal_flow_filenames:\n",
    "print('querying normal flows...')\n",
    "normal_flow_filenames = merged_flows.filename[merged_flows['category'] == 'Normal']\n",
    "\n",
    "# get data:\n",
    "print('processing pcaps...')\n",
    "process_and_save_data(\n",
    "    flow_path,\n",
    "    max_flows,\n",
    "    normal_flow_filenames,\n",
    "    images_path,\n",
    "    attack_str\n",
    "    )\n",
    "\n",
    "print('preprocessing completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e126a-9a69-4395-954d-410299f9b44e",
   "metadata": {},
   "source": [
    "# Optimized data preprocessing:\n",
    "_______________\n",
    "In the following code we just take into account the fact that we already removed the normal flows from the directories we are exploring, so we do not read too much metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e23ae77b-7ddd-4f52-ae5f-c5a377029031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WORKSPACE_DIR = '/home/jovyan/nfs/jcevallos/datasets/'\n",
    "# !tar -xzvf {datafile} -C {WORKSPACE_DIR}\n",
    "datadir = WORKSPACE_DIR + 'IIoT_Ferrag_raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fd52da4-96b3-44c3-aaf1-93c67839a244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing pcaps...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing completed!\n"
     ]
    }
   ],
   "source": [
    "images_path = '/home/jovyan/shared/jesus/datasets/BoT_IoT_imgs/'\n",
    "# CHANGE THIS FOR EACH ATTACK:\n",
    "attack_str = 'Ransomware'\n",
    "max_flows = 10000\n",
    "\n",
    "attack_dir = datadir + f'{attack_str}'\n",
    "flow_path = attack_dir + '_flows/'\n",
    "\n",
    "\n",
    "# get data:\n",
    "print('processing pcaps...')\n",
    "process_and_save_data_opt(\n",
    "    flow_path,\n",
    "    max_flows,\n",
    "    images_path,\n",
    "    attack_str\n",
    "    )\n",
    "\n",
    "print('preprocessing completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34f1fe7-31c1-47c8-9e5f-bd0ff489d78f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Optimized data processing 2.0 (images):\n",
    "________\n",
    "It turns out pytorch tensors are heavy :/ we will save and load our data as images and then convert themm to tensors in the dataloading..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e835b52-6a39-4abf-8b06-7472e5f54e86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing pcaps...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9273223d5dc49999244f22a9a6bbd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing completed!\n"
     ]
    }
   ],
   "source": [
    "WORKSPACE_DIR = '/home/jovyan/nfs/jcevallos/datasets/'\n",
    "# !tar -xzvf {datafile} -C {WORKSPACE_DIR}\n",
    "datadir = WORKSPACE_DIR + 'BoT_IoT_raw/'\n",
    "normal_traffic_dir = datadir + 'Normal/'\n",
    "images_path = '/home/jovyan/shared/jesus/datasets/optimized_BoT_IoT_imgs'\n",
    "# CHANGE THIS FOR EACH ATTACK:\n",
    "macro_attack_str = 'DDoS'\n",
    "attack_str = 'DDoS_UDP'\n",
    "max_flows = 10000\n",
    "\n",
    "attack_dir = datadir + f'{macro_attack_str}/{attack_str}/'\n",
    "flow_path = attack_dir + 'flows'\n",
    "\n",
    "\n",
    "# get data:\n",
    "print('processing pcaps...')\n",
    "process_and_save_image_opt(\n",
    "    flow_path,\n",
    "    max_flows,\n",
    "    images_path,\n",
    "    attack_str\n",
    "    )\n",
    "\n",
    "print('preprocessing completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479518e0-6b12-4b40-a955-b14b1d6e69a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
